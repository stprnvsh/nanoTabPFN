\documentclass[aspectratio=169]{beamer}
\usetheme{metropolis}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\definecolor{bgdark}{HTML}{1a1a2e}
\definecolor{accent}{HTML}{e94560}
\definecolor{accent2}{HTML}{0f3460}
\definecolor{textlight}{HTML}{eaeaea}
\definecolor{codebg}{HTML}{16213e}
\definecolor{codegreen}{HTML}{00ff88}

\setbeamercolor{background canvas}{bg=bgdark}
\setbeamercolor{normal text}{fg=textlight}
\setbeamercolor{frametitle}{fg=textlight,bg=accent2}
\setbeamercolor{title}{fg=textlight}
\setbeamercolor{progress bar}{fg=accent}
\setbeamercolor{alerted text}{fg=accent}

\lstset{
    basicstyle=\ttfamily\tiny,
    backgroundcolor=\color{codebg},
    keywordstyle=\color{accent},
    stringstyle=\color{codegreen},
    commentstyle=\color{gray},
    breaklines=true,
    frame=single,
    rulecolor=\color{accent2}
}

\title{Profiling Training Pipeline}
\date{}

\begin{document}

\begin{frame}
\titlepage
\end{frame}
\begin{frame}{nanoTabPFN}
\begin{itemize}
    \item NanoTabPFN experiments
    \item Hardware: NVIDIA A100-SXM4-80GB
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Baseline: train.py}
\begin{lstlisting}[language=Python]
with h5py.File(filename, "r") as f:
    for _ in range(num_steps):
        x = torch.from_numpy(f["X"][ptr:end])
        y = torch.from_numpy(f["y"][ptr:end])
        yield dict(
            x=x.to(device),
            y=y.to(device),
        )
\end{lstlisting}
\end{frame}

\section{Profiling}

\begin{frame}[fragile]{Profiling Setup}
\begin{lstlisting}[language=Python]
from torch.profiler import profile, ProfilerActivity

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
    record_shapes=True,
    profile_memory=True,
) as prof:
    model, history = train(model, prior, lr=4e-3)

print(prof.key_averages().table(sort_by="cuda_time_total"))
\end{lstlisting}
\end{frame}

\begin{frame}{Baseline Profile (100 steps, batch=6, 5000 rows)}
\begin{table}
\centering
\small
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total time & 68.75s \\
Steps/sec & 1.5 \\
ms/step & 687.51 \\
\midrule
Self CPU time total & 57.415s \\
Self CUDA time total & 50.156s \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Baseline: Top CPU Operations}
\begin{table}
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Self CPU} & \textbf{\% CPU} \\
\midrule
cudaMemcpyAsync & 44,084ms & 76.78\% \\
cudaMalloc & 7,081ms & 12.33\% \\
cudaLaunchKernel & 645ms & 1.12\% \\
aten::bmm & 180ms & 0.31\% \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Baseline: Top CUDA Operations}
\begin{table}
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Self CUDA} & \textbf{\% CUDA} \\
\midrule
aten::bmm & 32,509ms & 64.82\% \\
ampere\_sgemm\_128x128\_nn & 13,783ms & 27.48\% \\
ampere\_sgemm\_128x128\_nt & 13,706ms & 27.33\% \\
aten::\_softmax\_backward\_data & 3,880ms & 7.73\% \\
aten::mul & 3,397ms & 6.77\% \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Baseline: Memory Transfers}
\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{CPU Time} & \textbf{Calls} \\
\midrule
aten::to & 44,240ms & 7,362 \\
aten::\_to\_copy & 44,231ms & 462 \\
aten::copy\_ & 44,352ms & 12,824 \\
cudaMemcpyAsync & 44,084ms & 1,024 \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\section{Optimizations}

\begin{frame}{train\_optimized.py}
\begin{enumerate}
    \item Pinned memory
    \item Non-blocking transfers
    \item CUDA streams for overlap
    \item Double buffering (prefetch to VRAM)
    \item GPU Direct Storage (kvikio)
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{Pinned Memory + Non-blocking}
\begin{lstlisting}[language=Python]
# Before
x = torch.from_numpy(x_np).to(device)

# After
x = torch.from_numpy(x_np).pin_memory().to(device, non_blocking=True)
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{CUDA Streams + Double Buffering}
\begin{lstlisting}[language=Python]
self.transfer_stream = torch.cuda.Stream()

vram_buffer = [self._load_to_vram(f) for _ in range(prefetch)]

for step in range(num_steps):
    batch = vram_buffer.pop(0)
    
    with torch.cuda.stream(self.transfer_stream):
        next_batch = self._load_to_vram(f)
    vram_buffer.append(next_batch)
    
    torch.cuda.current_stream().wait_stream(self.transfer_stream)
    yield batch
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{GPU Direct Storage (kvikio)}
\begin{lstlisting}[language=Python]
import kvikio
import cupy as cp

x_gpu = cp.empty((batch_size, max_seq, num_features), dtype=cp.float32)
x_gpu[:] = cp.asarray(f["X"][ptr:end])
x = torch.as_tensor(x_gpu, device=self.device)
\end{lstlisting}
\end{frame}

\section{Results}

\begin{frame}{Optimized Profile (100 steps, batch=6, 5000 rows, GDS)}
\begin{table}
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Optimized} \\
\midrule
Total time & 68.75s & 45.30s \\
Steps/sec & 1.5 & 2.2 \\
ms/step & 687.51 & 453.00 \\
\midrule
Self CPU time total & 57.415s & 49.992s \\
Self CUDA time total & 50.156s & 30.691s \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Optimized: Top CPU Operations}
\begin{table}
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Self CPU} & \textbf{\% CPU} \\
\midrule
Command Buffer Full & 23,450ms & 46.91\% \\
cudaLaunchKernel & 10,733ms & 21.47\% \\
cudaMalloc & 5,607ms & 11.22\% \\
cuLaunchKernel & 3,249ms & 6.50\% \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Optimized: Top CUDA Operations}
\begin{table}
\centering
\footnotesize
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Self CUDA} & \textbf{\% CUDA} \\
\midrule
aten::bmm & 13,850ms & 45.13\% \\
aten::\_softmax\_backward\_data & 4,160ms & 13.55\% \\
Command Buffer Full & 3,975ms & 12.95\% \\
cutlass\_80\_tensorop\_s1688gemm & 3,723ms & 12.13\% \\
aten::mul & 3,650ms & 11.89\% \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Memory Transfer Comparison}
\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Operation} & \textbf{Baseline} & \textbf{Optimized} \\
\midrule
aten::to & 44,240ms & 109ms \\
cudaMemcpyAsync & 44,084ms & 268ms \\
aten::copy\_ & 44,352ms & 4,425ms \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{CUDA Memory Summary}
\begin{table}
\centering
\small
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Optimized} \\
\midrule
Peak Usage & 72,327 MiB & 72,326 MiB \\
Total Allocated & 11,394 GiB & 12,864 GiB \\
cudaMalloc retries & 5 & 5 \\
\bottomrule
\end{tabular}
\end{table}
\end{frame}

\end{document}
